{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Explore here"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[nltk_data] Downloading package stopwords to /home/vscode/nltk_data...\n",
                        "[nltk_data]   Package stopwords is already up-to-date!\n",
                        "[nltk_data] Downloading package punkt to /home/vscode/nltk_data...\n",
                        "[nltk_data]   Package punkt is already up-to-date!\n",
                        "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
                        "[nltk_data]     /home/vscode/nltk_data...\n",
                        "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
                        "[nltk_data]       date!\n",
                        "[nltk_data] Downloading package wordnet to /home/vscode/nltk_data...\n",
                        "[nltk_data]   Package wordnet is already up-to-date!\n"
                    ]
                }
            ],
            "source": [
                "import re\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from sklearn.svm import SVC\n",
                "from sklearn.svm import LinearSVC\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.model_selection import RepeatedStratifiedKFold\n",
                "from sklearn.model_selection import GridSearchCV\n",
                "from sklearn.metrics import make_scorer, confusion_matrix\n",
                "from sklearn.metrics import f1_score\n",
                "from sklearn.metrics import accuracy_score\n",
                "from sklearn.metrics import recall_score\n",
                "from sklearn.feature_extraction.text import CountVectorizer\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sklearn.metrics import classification_report\n",
                "\n",
                "import nltk\n",
                "nltk.download('stopwords')\n",
                "from nltk.corpus import stopwords\n",
                "stop_words = stopwords.words('english')\n",
                "from nltk.corpus import wordnet\n",
                "nltk.download('punkt')\n",
                "nltk.download('averaged_perceptron_tagger')\n",
                "nltk.download('wordnet')\n",
                "from nltk.stem import WordNetLemmatizer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [],
            "source": [
                "def grid_SVC(X_train, y_train, performance_metric='f1', resultsGrid=False):\n",
                "    model = SVC()\n",
                "    C = np.linspace(0.000001 , 1000, 10)\n",
                "    kernels = ['poly', 'rbf', 'linear', 'sigmoid']\n",
                "    gamma = ['scale', 'auto']\n",
                "    grid = dict(C = C, kernel = kernels, gamma = gamma)\n",
                "    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=1)\n",
                "    grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv,\n",
                "                           scoring=performance_metric,error_score='raise')\n",
                "    grid_result = grid_search.fit(X_train, y_train)\n",
                "    if resultsGrid==True:\n",
                "        return grid_result.cv_results_\n",
                "    else:\n",
                "        return  grid_result.best_estimator_\n",
                "\n",
                "\n",
                "def lemmatize_text(text):\n",
                "    tokens = text.split()\n",
                "    lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n",
                "    return ' '.join(lemmatized_words)\n",
                "\n",
                "\n",
                "\n",
                "def clean_text(string):\n",
                "    string = string.lower()\n",
                "    string = re.sub(r\"http(s)?:*\", '', string)\n",
                "    string = re.sub(r\"[-/.#&]\", ' ', string)\n",
                "    string = re.sub(r\"w{3}\", ' ', string)\n",
                "    string = string.strip()\n",
                "    string = ' '.join([word for word in string.split() if word not in stop_words])\n",
                "    string = lemmatize_text(string)\n",
                "    return string"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [],
            "source": [
                "url = \"https://raw.githubusercontent.com/4GeeksAcademy/NLP-project-tutorial/main/url_spam.csv\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [],
            "source": [
                "df  = pd.read_csv(url)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "Index(['url', 'is_spam'], dtype='object')"
                        ]
                    },
                    "execution_count": 26,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "df.columns"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [],
            "source": [
                "samples = [df['url'].loc[np.random.randint(0,df.shape[0])] for _ in range(30)]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "['nytimes com 2020 05 30 technology twitter trump dorsey html',\n",
                            " 'bbc com news uk england wiltshire 53132567',\n",
                            " 'morningbrew com daily story 2020 06 30 2020 photo',\n",
                            " 'hvper com',\n",
                            " 'youtube com watch?v=r9ozap my5e',\n",
                            " 'theverge com 2020 6 18 21296180 apple hey email app basecamp rejection response controversy antitrust regulation',\n",
                            " 'cosmopolitan com lifestyle a32980205 jenna marble quitting youtube blackface nicki minaj apology',\n",
                            " 'tiqets com blog black history museum',\n",
                            " 'nirandfar com focus quiz',\n",
                            " 'gimletmedia com show reply n8hwl7 128 crime machine part ii episode player',\n",
                            " 'briefingday com fan',\n",
                            " 'cato org publication commentary dc statehood fool errand',\n",
                            " 'food52 com',\n",
                            " 'usatoday com story news politics election 2020 06 23 june 23 primary aoc win charles booker amy mcgrath result delayed 3235505001',\n",
                            " 'wired co uk article coronavirus beijing second wave lockdown',\n",
                            " 'vox com policy politics 2020 6 24 21281485 bill barr donald trump berman doj david rohde',\n",
                            " 'nikolehannahjones com',\n",
                            " 'doc google com form e 1faipqlsdkbw zwn5oocqkxg3xlccpgrydnzdghbb0hhshpnq59fkcrw viewform',\n",
                            " 'theverge com 2020 6 29 21304947 reddit ban subreddits donald chapo trap house new content policy rule',\n",
                            " 'businesscasual fm',\n",
                            " 'cnn com 2020 06 30 entertainment car reiner obit index html',\n",
                            " 'reddit com r aww comment hiz3bh squirrel_is_living_the_best_life',\n",
                            " 'thedailybeast com trump official didnt want tell russian bounty',\n",
                            " 'numlock substack com p 2019 sunday special',\n",
                            " 'dameproducts com page skimm 1',\n",
                            " 'king5 com article news local seattle amazon rename keyarena climate pledge arena securing naming right 281 e5381d4a c150 4919 8b01 ee632e95cc61',\n",
                            " 'reuters com article u health coronavirus usa wave explainer second wave pandemic arrived u iduskbn23u396',\n",
                            " 'profgalloway com higher ed enough already',\n",
                            " 'shop perfectketo com product keto cooky',\n",
                            " 'interactioninstitute org temporary autonomous zone']"
                        ]
                    },
                    "execution_count": 28,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "lemmatizer = WordNetLemmatizer()\n",
                "stop_words.extend(['of', 'yet'])\n",
                "list(map( clean_text, samples))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {},
            "outputs": [],
            "source": [
                "X, y = df['url'], df['is_spam']"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_train, X_test, y_train, y_test = train_test_split(X,y,\n",
                "                                                    shuffle=True,\n",
                "                                                    test_size = 0.3,\n",
                "                                                    random_state=123)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "1099    https://www.reuters.com/article/us-usa-trump-o...\n",
                            "1448    https://www.eventbrite.com/e/big-friendship-bo...\n",
                            "2327    https://news.rice.edu/2020/06/29/laser-welded-...\n",
                            "1412    https://creativemornings.com/companies/sdco-pa...\n",
                            "1224    https://www.nytimes.com/2020/06/25/world/afric...\n",
                            "                              ...                        \n",
                            "1147    https://en.wikipedia.org/wiki/Tim_O%27Brien_(a...\n",
                            "2154    https://www.washingtonpost.com/privacy-policy/...\n",
                            "1766    https://www.cnbc.com/2020/06/26/amazon-buys-se...\n",
                            "1122    https://www.amazon.com/Rivers-Tides-Andy-Golds...\n",
                            "1346            https://www.gao.gov/assets/710/707839.pdf\n",
                            "Name: url, Length: 2099, dtype: object"
                        ]
                    },
                    "execution_count": 31,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "X_train"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {},
            "outputs": [],
            "source": [
                "# cleaning data\n",
                "X_train = X_train.apply(lambda x : clean_text(x))\n",
                "X_test = X_test.apply(lambda x : clean_text(x))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "1099    reuters com article u usa trump obamacare trum...\n",
                            "1448    eventbrite com e big friendship book launch ti...\n",
                            "2327    news rice edu 2020 06 29 laser welded sugar sw...\n",
                            "1412            creativemornings com company sdco partner\n",
                            "1224    nytimes com 2020 06 25 world africa ebola cong...\n",
                            "                              ...                        \n",
                            "1147         en wikipedia org wiki tim_o%27brien_(author)\n",
                            "2154    washingtonpost com privacy policy 2011 11 18 g...\n",
                            "1766    cnbc com 2020 06 26 amazon buy self driving te...\n",
                            "1122    amazon com river tide andy goldsworthy dp b001...\n",
                            "1346                         gao gov asset 710 707839 pdf\n",
                            "Name: url, Length: 2099, dtype: object"
                        ]
                    },
                    "execution_count": 33,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "X_train"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.feature_extraction.text import CountVectorizer\n",
                "vect = CountVectorizer().fit(X_train)\n",
                "X_train = vect.transform(X_train)\n",
                "X_test  = vect.transform(X_test)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {},
            "outputs": [],
            "source": [
                "y_train = np.where(y_train==True,1,0)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "array([0, 1, 0, ..., 0, 0, 0], shape=(2099,))"
                        ]
                    },
                    "execution_count": 36,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "y_train"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "metadata": {},
            "outputs": [],
            "source": [
                "best_ml = grid_SVC(X_train, y_train)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "metadata": {},
            "outputs": [],
            "source": [
                "preds = best_ml.predict(X_test)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "              precision    recall  f1-score   support\n",
                        "\n",
                        "       False       0.97      0.96      0.97       682\n",
                        "        True       0.87      0.92      0.89       218\n",
                        "\n",
                        "    accuracy                           0.95       900\n",
                        "   macro avg       0.92      0.94      0.93       900\n",
                        "weighted avg       0.95      0.95      0.95       900\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "print(classification_report(y_test, preds))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "El modelo SVM obtuvo un 95% de accuracy, mostrando un buen desempeño en la detección de URLs spam. El alto recall en la clase spam (92%) indica que el modelo identifica correctamente la mayoría de los enlaces maliciosos, siendo adecuado para este problema de clasificación."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.4"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
